# -- Cluster ceph.conf override
configOverride: |
  [global]
  osd_pool_default_size = 1
  mon_warn_on_pool_no_redundancy = false
toolbox:
  enabled: true
  resources:
    limits:
      memory: "1Gi"
    requests:
      cpu: null
      memory: "128Mi"  
monitoring:
  enabled: true
  createPrometheusRules: true
# Specify these values to override the Ceph image in the cephClusterSpec below.
# If specifying these values, do not include the cephVersion section in the cephClusterSpec.
cephImage:
  # The repository from which to pull the ceph image
  repository: quay.io/ceph/ceph
  # In production, use a specific version tag instead of the general v19 flag, which pulls the latest release and could result in different
  # versions running within the cluster. See tags available at https://hub.docker.com/r/ceph/ceph/tags/.
  # To be more precise, you can always use a timestamp tag such as quay.io/ceph/ceph:v19.2.3-20250717
  tag: v19.2.3
  # Whether to allow unsupported versions of Ceph. Currently Reef and Squid are supported.
  # Future versions such as Tentacle (v20) would require this to be set to `true`.
  # Do not set to true in production.
  allowUnsupported: false
  # The image pull policy for pulling the ceph image in the ceph daemon pods, defaults to IfNotPresent
  # imagePullPolicy: IfNotPresent
cephClusterSpec:
  mon:
    count: 1
  mgr:
    count: 2
    allowMultiplePerNode: true
    modules:
      - name: rook
        enabled: true
  dashboard:
    enabled: true
    prometheusEndpoint: http://rook-prometheus:9090
    prometheusEndpointSSLVerify: false
    # urlPrefix: /ceph-dashboard
    port: 8080
    ssl: false
  network:
    connections:
    provider: multus
    selectors:
      public: public-net
      cluster: cluster-net
  annotations:
  #   all:
  #   mon:
  #   osd:
  #   cleanup:
  #   prepareosd:
  #   # If no mgr annotations are set, prometheus scrape annotations will be set by default.
  #   mgr:
    dashboard:
      omni-kube-service-exposer.sidero.dev/label: Ceph Dashboard
      omni-kube-service-exposer.sidero.dev/port: "50082"
      omni-kube-service-exposer.sidero.dev/prefix: cephdash
      omni-kube-service-exposer.sidero.dev/icon: H4sICGfoVmgAA2NlcGhfbG9nb19pY29uXzE3MDQwNi5zdmcAZVbtTmQ3DH2VaPr7hsR24qQCpJbfPMRo2GWQZrurgpZ9/J7jXDqoFZIJSa4/jo9PuH39+Zx+fbv89Xp3OL+9/fj95ub9/T2/a/7+9/ONlFJucOOQ3l+e3s53h26HdP7y8nx+W+ufL1/e//z+6+6waRZJm2VLvWd35S/VcUhfXy6Xu8NvX4uJPR3ub38c387p6e7w2Gp28dRa9qmnTbKJbzUrXI2q+Nutw2EVGJW+ZZkd502zDmc8lSS5jLG1LMWS5dIdd3vvCbELnJV8Xfc8xkPruRRJFV8kQ7JdkvKOliwjlmfE0QdcGNzmGaLr/gVXMhFfj9JyH2nZEj+MM1LEqSfUVluqeY6GJDV818F8Z08tVzXWwxvNB6rqw3ClNGPFGqGmoEjHjmJn81ycFjG2SVAqPsWHFUCJczV7Y/VwJ7mZ4G5FkDYJZVPZ8gRQ1RoXnr23owJpABCWFVTkBwgrsfU/uD/TsuuU4A1kr1bj25aWXadAe5QO1zL6g1TeZQFCkKKTBABAl7bWZ0ShGx9p2d0No4/IRB/Mc+tsPSABX6QAfxKtkRIWqdDFrGnZ5YIp+J5PpFh3ey1SVxxPCrzHbuN4Q72SAqAT4POO+9YVAJuiLaBF4Ksp8EVDzbFdpwFd7ZZGVjSKLUIM9Kjoo05675Jro0+d6IGwZW600sh5QAfjpLh0iUOJIRiES7lGbwYcCiEEqTS3SlzbRLHZNswd8nMcMTZsJZtKJ/kc2fm45IL6CkpXpGClnTLA7cFDqeShVUwHuMtw0uSICjACyy6aY0Jri3HHPoLYbtcpwnXOY3MWqiyvDdREEgtBnOy+oSi0i1XKIIhksRVS3XTEbI8jM1m8LERAeoxJYzJDd7tOsQ+WIwCH14xJz7gxP9/ArEhszt1+cp3C9Ynyow7HzKlBEZhvZMXpXEllDnnYD9eAL0oKIOpu/4WDA0M42FX4A7iFlAJhAkpNAWW2CeHgViWiuTWPPgTfm1zQMCLUMFXOBh4rOJXCLFKjxRgONvkEDlB0wo8GabmCMJMfhWwYFForzi3SuSPLYBYBYJZDNLYdfoZ40JOZgp8XVuwhLP8fPbS+d5LTwMkJwi6zwEDyaPZgqadCAjdqUV1yEIIv5DAIKjSUrMIrpYEtxbkaFwQOVWOqpBE7249SMpCiWWmgY50qZwAB26a73Se8gnoDOi3xje/2A0jFkEB31r592HWKd6FwluD+hAo6SWlOSS8hvFAKCENwerTIq8y07If4DIx7FqjfOq8pzPXU8kqr7fZD04at834iJchNQzNxGwAj6UiNZDMyBwCROSw8u5KgVGuUxvRQGiNHgHbFRNMVk/+A6VREgImhxrMJzeB8YZ6CR1QPNIGtEWiSc/RL4WPBltVdc/A33yOqVzQWwrnaiq5SGEZQPVjA128sQpChhdIDx1HzdL7o4GfQibQFnYKI8RrN9omIId3TF2OVjH3E+1M7bmLg5MTnCRPcOWekeJV9Hfb1uhOzs++vNf9vwOm+jv1j3/PdH9PteqaHm/tb/iN1/w8KodprcAkAAA==
  resources:
    mgr:
      limits:
        memory: "1Gi"
      requests:
        cpu: null
        memory: "512Mi"
    mon:
      limits:
        memory: "2Gi"
      requests:
        cpu: null
        memory: "1Gi"
    osd:
      limits:
        memory: "4Gi"
      requests:
        cpu: null
        memory: "4Gi"
    prepareosd:
      # limits: It is not recommended to set limits on the OSD prepare job
      #         since it's a one-time burst for memory that must be allowed to
      #         complete without an OOM kill.  Note however that if a k8s
      #         limitRange guardrail is defined external to Rook, the lack of
      #         a limit here may result in a sync failure, in which case a
      #         limit should be added.  1200Mi may suffice for up to 15Ti
      #         OSDs ; for larger devices 2Gi may be required.
      #         cf. https://github.com/rook/rook/pull/11103
      requests:
        cpu: null
        memory: "50Mi"
    mgr-sidecar:
      limits:
        memory: "100Mi"
      requests:
        cpu: null
        memory: "40Mi"
    crashcollector:
      limits:
        memory: "60Mi"
      requests:
        cpu: null
        memory: "60Mi"
    logcollector:
      limits:
        memory: "1Gi"
      requests:
        cpu: null
        memory: "100Mi"
    cleanup:
      limits:
        memory: "1Gi"
      requests:
        cpu: null
        memory: "100Mi"
    exporter:
      limits:
        memory: "128Mi"
      requests:
        cpu: null
        memory: "50Mi"
  priorityClassNames:
    mon: system-node-critical
    osd: system-node-critical
    mgr: system-cluster-critical      
cephBlockPools:
  - name: ceph-blockpool
    spec:
      failureDomain: host
      replicated:
        size: 1
      # deviceClass: hdd
      deviceClass: ssd
    storageClass:
      enabled: true
      name: ceph-block
      annotations: {}
      labels: {}
      isDefault: true
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      volumeBindingMode: "Immediate"
      mountOptions: []
      allowedTopologies: []
      parameters:
        imageFormat: "2"
        imageFeatures: layering
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
        csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
        csi.storage.k8s.io/fstype: ext4
  - name: ceph-blockpool-fast
    spec:
      failureDomain: host
      replicated:
        size: 1
      deviceClass: ssd
    storageClass:
      enabled: true
      name: ceph-block-fast
      annotations: {}
      labels: {}
      isDefault: false
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      volumeBindingMode: "Immediate"
      mountOptions: []
      allowedTopologies: []
      parameters:
        imageFormat: "2"
        imageFeatures: layering
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
        csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
        csi.storage.k8s.io/fstype: ext4
cephFileSystems:
  - name: ceph-filesystem
    spec:
      metadataPool:
        failureDomain: host
        replicated:
          size: 1
        # deviceClass: hdd
        deviceClass: ssd
      dataPools:
        - failureDomain: host
          replicated:
            size: 1
          name: data0
          # deviceClass: hdd
          deviceClass: ssd
      metadataServer:
        activeCount: 1
        activeStandby: true
        resources:
          limits:
            memory: "4Gi"
          requests:
            cpu: null
            memory: "4Gi"
        priorityClassName: system-cluster-critical
    storageClass:
      enabled: true
      isDefault: false
      name: ceph-filesystem
      pool: data0
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      volumeBindingMode: "Immediate"
      annotations: {}
      labels: {}
      mountOptions: []
      parameters:
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
        csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
        csi.storage.k8s.io/fstype: ext4
cephObjectStores:
  - name: ceph-objectstore
    spec:
      metadataPool:
        failureDomain: host
        replicated:
          size: 1
        # deviceClass: hdd
        deviceClass: ssd
      dataPool:
        failureDomain: host
        replicated:
          size: 1
        parameters:
          bulk: "true"
        # deviceClass: hdd
        deviceClass: ssd
      preservePoolsOnDelete: true
      gateway:
        port: 80
        resources:
          limits:
            memory: "2Gi"
          requests:
            cpu: null
            memory: "1Gi"
        instances: 1
        priorityClassName: system-cluster-critical
        opsLogSidecar:
          resources:
            limits:
              memory: "100Mi"
            requests:
              cpu: null
              memory: "40Mi"
    storageClass:
      enabled: true
      name: ceph-bucket
      reclaimPolicy: Delete
      volumeBindingMode: "Immediate"
      annotations: {}
      labels: {}
      parameters:
        region: us-west-1
    ingress:
      enabled: false